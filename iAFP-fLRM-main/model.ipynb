{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5cd97f-c070-4b4d-90d9-1fcb300796f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import AdaptiveAvgPool1d\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_hidden_dim = 64\n",
    "        self.lstm_hidden_dim = 32\n",
    "        self.MLP_embed_dim = 64\n",
    "        self.MLP_hidden_dim = 64\n",
    "        self.dropout = 0.2\n",
    "        self.batch_size = 64\n",
    "        self.emb_dim = 128\n",
    "        self.embedding_seq = nn.Embedding(24, self.emb_dim, padding_idx=0, device=device)\n",
    "        self.encoder_layer_seq = nn.TransformerEncoderLayer(d_model=self.emb_dim, nhead=8)\n",
    "        self.transformer_encoder_seq = nn.TransformerEncoder(self.encoder_layer_seq, num_layers=1)\n",
    "        self.conv_seq = nn.Sequential(\n",
    "            nn.Conv1d(128, 20, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        )\n",
    "        self.conv_HF = nn.Sequential(\n",
    "            nn.Conv1d(128, 20, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        )\n",
    "        self.linearlayer = nn.Linear(1, 128, device=device)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(20, self.lstm_hidden_dim, num_layers=2, bidirectional=True, dropout=self.dropout,\n",
    "                            batch_first=False, device=device)\n",
    "\n",
    "        self.block1 = self._make_res_block(self.MLP_embed_dim, self.MLP_hidden_dim).to(device)\n",
    "        self.block2 = self._make_res_block(self.MLP_hidden_dim, self.MLP_hidden_dim).to(device)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.MLP_hidden_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "        self.branch_attn = nn.MultiheadAttention(\n",
    "            embed_dim=20,\n",
    "            num_heads=5,\n",
    "            batch_first=True,\n",
    "            dropout=0.0\n",
    "        ).to(device)\n",
    "        self.attn_scale = nn.Parameter(torch.ones(1) * 0.1)\n",
    "\n",
    "        self.adaptive_pool = AdaptiveAvgPool1d(1)\n",
    "    def _make_res_block(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pos_embed, HF):\n",
    "\n",
    "        output1 = self.embedding_seq(x) + pos_embed.to(device)\n",
    "        output1 = self.transformer_encoder_seq(output1)\n",
    "        output1 = output1.permute(0, 2, 1)\n",
    "        output1 = self.conv_seq(output1)  # (batch, 20, seq1)\n",
    "\n",
    "        output2 = HF.unsqueeze(2)\n",
    "        output2 = output2.to(device)\n",
    "        output2 = self.relu(self.linearlayer(output2))\n",
    "        output2 = output2.permute(0, 2, 1)\n",
    "        output2 = self.conv_HF(output2)  # (batch, 20, seq2)\n",
    "\n",
    "        seq_len1 = output1.size(2)\n",
    "        seq_len2 = output2.size(2)\n",
    "        target_len = min(seq_len1, seq_len2)\n",
    "\n",
    "        if seq_len1 > target_len:\n",
    "\n",
    "            self.adaptive_pool.output_size = target_len\n",
    "            output1 = self.adaptive_pool(output1)\n",
    "        else:\n",
    "\n",
    "            output1 = output1[:, :, :target_len]\n",
    "\n",
    "        if seq_len2 > target_len:\n",
    "            self.adaptive_pool.output_size = target_len\n",
    "            output2 = self.adaptive_pool(output2)\n",
    "        else:\n",
    "            output2 = output2[:, :, :target_len]\n",
    "\n",
    "        q = output1.permute(0, 2, 1)\n",
    "        k = v = output2.permute(0, 2, 1)\n",
    "\n",
    "        attn_out, attn_weights = self.branch_attn(q, k, v)\n",
    "        attn_out = attn_out * self.attn_scale\n",
    "        fused = q + attn_out\n",
    "        fused = fused.permute(0, 2, 1)\n",
    "\n",
    "        output = torch.cat([output1, fused], 2)\n",
    "        output = output.permute(2, 0, 1)\n",
    "        output, (h, c) = self.lstm(output)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        identity = output.mean(dim=1)\n",
    "        out = self.block1(output.mean(dim=1)) + identity\n",
    "        out = self.block2(out) + out\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out, attn_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
